- $u+v \in V$ (Sum defined)
- $u+v=v+u$ (commutative)
- $(u+v)+w=u+(v+w)$ 
- $cu \in V$ (Scalar Multiples Exist)
- $c(u+v)=cu+cv$
- $(c+d)u=cu+cd$
- $c(du)=(cd)u$
- $1 \cdot u = u$ (Multiplicative Identity)
- There is a zero vector $0 \in V$ such that $u+0=u$ (Additive 0)
- There is a vector $-u \in V$ such that $u + (-u)=0$ (Additive inverse)

#### Example 
$\mathbb{R}^n$ satisfies all these properties.
Set of all real valued functions. (Yes, they can also be thought of vector spaces)
	Sum of two real valued functions would be a real valued function 
Set of all complex valued functions.

Hence, the vector space can be **Real Valued** or **Complex Valued**. In this course, our main focus is in the $n$-dimensional vector space.

---
## Subspace 
#def A **Subspace** of a vector space V is a subset H of V if it satisfies ($H \subseteq V$)
- The zero vector of V is in H
- $u,v \in H \implies u+v \in H$ (closed under vector addition)
- $c \in \mathbb{R}, \space u \in H \implies cu \in H$ (closed under scalar multiplication)
- And so on...
- **Basically, a subspace must also be a vector space!**

#### Example 
$\set{0}$ is a complete subspace called the Zero-Subspace for any space $V$
	$\set{0} \subseteq V$ for any set $V$
	$0 \in \set{0}$
	$0+0=0, \space 0 \in \set{0}$
	$c0 = 0, \space 0 \in \set{0}, \space c \in \mathbb{R}$


Theoretically, the set of all locations that a car can reach from the origin is a subspace $H$ of $\mathbb{R}^2$.
	$H \subseteq \mathbb{R}^2$ (We *defined* the car to only move in $\mathbb{R}^2$)
	$0 \in H$ (Obviously the car can reach this by not moving)
	$u,v \in H, \space u+v \in H$
		The car's motion is defined by vectors. ![[Maths/Intro to Linear Algebra/attachments/Drawing 24-10-16-10-57-48]]
		If it can move along vectors $u,v$ then by first moving $u$ and then $v$, it can also move along $u+v$.
	$cu \in H, \space c \in \mathbb{R}$


The solution set of $A_{m \times n} x_{n \times 1} = 0$ is a subspace of $\mathbb{R}^k$
	$u \in H \implies Au = 0 \implies u \in \mathbb{R}^{n} \implies H \subseteq \mathbb{R}^n$
		all values of $x_{n \times 1}$ are in space $\mathbb{R}^n$
	$A0_{n \times 1}=0 \implies 0 \in H$
		0 vectors are trivial solutions of $Ax=0$ and are hence included in the solution set
	$u, v \in H \implies Au=0, Av=0$ 
		using properties of Matrix vector multiplication, $A(u+v)=Au+Av=0 \implies u+v \in H$
	$c \in \mathbb{R}, u \in H \implies Au=0$
		using properties of Matrix vector multiplication, 
		$A(cu)=c(Au)=c(0)=0 \implies cu \in H$

---
### Proving whether a space is not a Vector Space/Subspace
If $H$ is NOT a subspace of a vector space, (i.e. if H is not a vector space on its own),
any one counterexample or any proof for any of the properties is enough.

#### Example 
$\begin{bmatrix}x \\ y \end{bmatrix} \in \mathbb{R}^{2}:x^{2}+y^{2}=1$
![[Maths/Intro to Linear Algebra/attachments/Drawing 24-10-16-11-12-49]]
	Very simply, (there are a million ways to prove this, the following is just one of them),
		$0 \notin H$
		Hence it is not a subspace

$\begin{bmatrix}x \\ y \end{bmatrix} \in \mathbb{R}^{2}: x^{2}=y^{2}$
![[Maths/Intro to Linear Algebra/attachments/Drawing 24-10-16-11-14-46]]
	$u=\begin{bmatrix}1\\1\end{bmatrix} \in H$ and $v=\begin{bmatrix}1\\-1\end{bmatrix} \in H$
	But! $u+v = \begin{bmatrix}2\\0\end{bmatrix} \notin H$
	Hence not a subset 

$\begin{bmatrix}x \\ y \end{bmatrix} \in \mathbb{R}^{2}:x \leq 0, y \leq 0$
![[Maths/Intro to Linear Algebra/attachments/Drawing 24-10-16-11-18-09]]
	$v=\begin{bmatrix}-1\\-1\end{bmatrix} \in H$ but $-v=\begin{bmatrix}1\\1\end{bmatrix}\notin H$
	Hence not a subspace 

---
# Vector Spaces and Span
#theorem If $v_{1}, \dots v_{n}$ are in a vector space V, then $Span \set{v_{1}, \dots v_{n}}$ is a subspace of V.
>The space that vectors of a vector space span must be a subspace of the entire vector space

Spans are subspaces and Subspaces are spans

- For any subspace H of V, there exists a subset S of V, such that $H=span(S)$.
	S is the generating set of H.

So if a set can be written as a span of vectors of a space, it can be proven to be the subspace.

- $H = \set{\begin{bmatrix}2x+y \\ -x+3y\end{bmatrix} \in \mathbb{R}^{2}: x,y \in \mathbb{R}}$

Hence, in the parametric vector form, $H = \set{x \begin{bmatrix}2\\-1\end{bmatrix} + y \begin{bmatrix}1\\3\end{bmatrix}: x,y \in \mathbb{R}} = Span \set{\begin{bmatrix}2\\-1\end{bmatrix}, \begin{bmatrix}1\\3\end{bmatrix}} = Span(S)$
$S$ is the generating set of $H$


- $\set{\begin{bmatrix}3x+y\\ -x \\ 2y\end{bmatrix} \in \mathbb{R}^{3}: x,y \in \mathbb{R}}$
$H = Span(S), \enspace S = \set{\begin{bmatrix}3\\-1\\0\end{bmatrix}, \begin{bmatrix}1\\0\\-2\end{bmatrix}}$
$S$ is the generating set of $H$.

Note, we don't really have to prove something any more than this, given that we can **prove that a set $H$ is the span of vectors** in element of a vector space $V$ (eg: $\mathbb{R}^2$), we know directly from the theorem that it is indeed a subspace of the space $V$.

# Subspaces associated with a Matrix 
#def A **column space** of an $m \times n$ matrix $A$ is the span of its columns $ColS(A) = \text{Span \{columns of A\}}$

#theorem $Col(A_{m \times n})$ is a subspace of $\mathbb{R}^m$, $m = \text{\# of rows of A}$ 
$\begin{bmatrix}a_{1}& \dots & a_{n}\end{bmatrix}_{m \times n} \implies Col(A) \subseteq \mathbb{R}^m$

### Example 
Let $A = \begin{bmatrix}1&2&0&3\\0&0&1&2\\0&0&0&0\end{bmatrix}$
$Col(A)$ is the $Span \set{a_1,a_2,a_3,a_4}=SpanS \implies$ S is a generating set for $Col(A)$.
- $Col(A_{m \times n}) = Span \set{\text{cols of A}} = Range(T:x \rightarrow Ax)$
		$= \set{\text{b: Ax=b for some }x \in \mathbb{R}^{n}}$
- $b \in Col(A) \iff Ax=b$ is consistent $\iff$ b is in the range of $T$.

#### Another example
Is $u = \begin{bmatrix}3\\2\\1\end{bmatrix}$ in $Col(A)$? Is $v=\begin{bmatrix}2\\1\\0\end{bmatrix}$ in $Col(A)$? Is $w=\begin{bmatrix}1\\0\\0\end{bmatrix}$ in $Col(A)$?


Here we just solve $[A|u], [A|v], [A|w]$ and see if the systems are consistent. If they are, the respective matrices are in the Span of Columns of $A$.

# Row Space 
#def The **row space** of an $m \times n$ matrix A is the span of its rows.
$Row(A)= Span \set{rows \space of \space A} = Span \set{rows^{T} \space of \space A}$  
i.e. Span of Rows in form of column Vectors

(we take the transpose since we usually like to deal with spans of columns but they are literally the same thing.)

$A=\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix} \implies Row(A) = Span \set{\begin{bmatrix}1\\2\\3\end{bmatrix}, \begin{bmatrix}4\\5\\6\end{bmatrix}}$
notice how $Row(A) = Col(A^{T})$ since that is literally what the relation between $row^T$ and $columns$ is.

#theorem $Row(A_{m \times n})$ is a subspace of $\mathbb{R}^{n}$
$n = \text{\# columns of A}$

#def The **Null Space** of an $m \times n$ matrix A is
$Null(A) = \set{x:Ax=0}$
- $u \in Null(A) \iff Au=0$
- $Null(A) = \set{x:Ax=0} = \set{x:T(x)=0 \text{, where }T:x \rightarrow Ax}$
	- $\set{x:T(x)=0 \text{, where }T:x \rightarrow Ax}$ is also called the $Kernel(T)$.

#theorem $Null(A_{m \times n})$ is a subspace of $\mathbb{R}^n$, n is the number of columns of A.

To find the generating set of the null space, we just need to find the solution set of $Ax=0$
(Parametric Vector Form)

So a Null Space can be written as the parametric vector form of the solution set of $Ax=0$ as $\set{x_{1}v_{1}+x_{2}v_{2}, \space x_{1},x_{2} \in \mathbb{R}}$
which is also the $Span\set{v_{1},v_{2}}=SpanS$
Hence $\set{v_{1}, v_{2}}$ must be the generating set of the Null Space


If span of columns/rows of two matrices are the same, then the column/row space of the matrices are the same.

---

# Linearly Independent Sets and Bases
- $Span \set{v_{1}, \dots v_{k},u}= Span \set{v_{1}, \dots v_{k}}=H$ (subspace)
	- if $u$ is a linear combination of $v_{1}, \dots v_{k}$, then u is redundant and removing it from the set, the span does not change.
	- If the Generating Set is Linearly Independent, there are no redundant elements removing which the Space does not change 

These Linearly Dependent Sets are called Basis!
#def
Let H be a subspace of a vector space V. A set of vectors B in  V is a **basis** of H if
- B is linearly independent 
- H = Span(B)
- 


$\set{\begin{bmatrix}1\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\end{bmatrix}}$ Is called the **STANDARD BASIS**




---
#theorem  
we know
- $A_{m \times n}$ has a pivot position in each column $\iff$ columns of A are linearly independent 
- $A_{m \times n}$ has a pivot position in each row $\iff$ $Span \set{\text{columns of A}} = \mathbb{R}^{m}$
Let set $S = \set{\text{columns of A}}$
If A has a pivot position in each column and each row ($\iff$ $\cases{n(columns)=n(rows)=m\\\text{A is invertible}}$), then 
- $S$ is linearly independent generating set of $\mathbb{R}^{m}$
- i.e. S is a set of **basis vectors**.

**Any linearly independent set with exactly m vectors is a basis for $\mathbb{R}^m$.



---
The basic Idea is, that a **vector space** is a **set of Vectors** that contains certain vectors that pertain to certain nice properties.

It just so happens that a Span of Vectors, i.e. collection of all vectors that are linear combination of vectors happen to be a Vector Spaces.

So, **every vector space is a span and every span is a vector space**. Vector spaces and Spans define the same mathematical group of objects.

It's like how all linear transformations are just matrix transformations and vice versa. These are just different terms for the same definition.

How Linear transformations have a Standard matrix associated with them, a Vector Space has a **Generating Set** (of vectors) associated with it whose span is the Vector space.

Generating Sets can be columns/rows of a matrix, in which case they are called the **Column/Row spaces** of a matrix. These are just spans of the Columns/Rows of a Matrix.

A **Null Space/Kernel** is the Solution set of a $Ax=0$, i.e. it is the Space/Set of vectors that all transform to the Null vector when transformed by the Matrix.

Vectors of a linearly Independent Generating set of a vector space (i.e. any set of linearly independent vectors actually) are called **Bases Vectors** for the particular vector space. For a set to be the basis vectors of a space, they must span the entire space and they must be linearly independent.


### Example 
Is $\set{\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}2\\3\\0\end{bmatrix}}$ a basis for $\mathbb{R}^3$?
No! Although they are linearly independent, they do not span $\mathbb{R}^3$
Note how $\begin{bmatrix}1&2&x_1\\0&3&x_2\\0&0&x_3\end{bmatrix}$
would be consistent for all $x_1,x_2$ but the $x_3$ makes it inconsistent.
Hence, the given vectors are Basis vectors for $\mathbb{R}^2$ but NOT $\mathbb{R}^3$.

Quite obviously, given our prior knowledge, if there aren't $n$ vectors, we can not span the space $\mathbb{R}^n$. Immediately, seeing we only have 2 vectors, we can know it by no means spans $\mathbb{R}^3$ hence it can not be the generating set, let alone the linear dependency and consistency.


Adding a new vector to a set of linearly independent vectors that aren't yet the generating set can convert it to the basis. (given the additional vector is not a linear combination of our vectors).

# Bases for Null spaces
#theorem The vectors that multiply the free variables in the solution of $Ax=0$ in the parametric vector form form the basis of a Null(A).

> So a Null Space can be written as the parametric vector form of the solution set of $Ax=0$ as $\set{x_{1}v_{1}+x_{2}v_{2}, \space x_{1},x_{2} \in \mathbb{R}}$
which is also the $Span\set{v_{1},v_{2}}=SpanS$
Hence $\set{v_{1}, v_{2}}$ must be the generating set of the Null Space

Quoting from above, this is literally what it means, $\set{v_{1},v_{2}}$ would be the basis of the Null set where $V_1,v_2$ are the vectors we multiply by our free variables. (The vectors defining our freedom to move and span a space).

Since we consider other dependencies, this set is linearly independent and hence is not just the generating set but also the set of Basis Vectors for the Null Space of $A$.

(We can prove linear independence since there is only 1 element corresponding to the rows of the free variable and since $Sx=0$, there is a unique solution to $x=0$ and it is hence linearly independent)

---
# Bases from a Generating Set (Reduction Theorem)
A generating set is a set of vectors whose span is a vector space. It **Might** be a linearly dependent system of a linearly independent system. If it has linear in dependency, it's called the **Basis**.

To remove redundant vectors that cause dependency, we can convert $A$ to $REF(A)$ by only row operations.
[[Maths/Intro to Linear Algebra/Lec 5. Solution Sets of Linear Systems; Linear Independence#^826bcf|We know]] that all non pivot columns (corresponding to the free variables) lead to linear dependency and are redundant.

Removing the vectors corresponding to the non-pivot columns of REF(A) from the generating set would make the generating set linearly independent and would hence turn it into a Basis.

NOTE!
Columns of $REF(A)$ would NOT give you a correct Column/Row basis since they are defined to have a generating set made of columns/(transpose of)rows of A. You need to take only the information about position of non-pivot columns from the REF. NOT the columns themselves! The columns would only be from the original generating set!
S must be a finite generating set for a nonzero space H for it to be a Basis since any infinite generating set would be linearly dependent and would never be a basis.
---
# Extension Theorem
Again, 
Is $\set{\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}2\\3\\0\end{bmatrix}}$ a basis for $\mathbb{R}^3$?

No! $S$ is linearly independent. It is a subspace of $\mathbb{R}^3$ but it is not the generating set of $\mathbb{R}^3$ since it doesn't span all the space.

So the idea is, for any set $S$ of linearly independent vectors and is a subspace of Space $H$, can be converted into the Basis for space $H$ by adding vectors outside of Span of $S$ (but within that of $H$) while maintaining linear in dependency.

---
# Coordinate Systems 
- Vector space $V$ with basis $B = \set{b_1,\dots,b_n}$ is a subspace of $R_n$, they have similar structure of properties.


# Unique Representation Theorem 
- Let $B = \set{b_1,\dots,b_n}$ be a basis for a vector space $V$. Then for each $x$ in $V$, there exists a unique set of scalars $c_1,\dots,c_n$ such that $x=c_1b_1,\dots,c_nb_n$.

I.e. you can write a point in space $V$ as a unique linear combination of its bases vectors.
(Why unique? Because we defined bases to be linearly independent!)

$x = \begin{bmatrix}b_{1} & \dots b_{n}\end{bmatrix}\,\begin{bmatrix}c_{1}\\\vdots\\c_{n}\end{bmatrix}=\begin{bmatrix}b_{1} & \dots b_{n}\end{bmatrix}\,\begin{bmatrix}x\end{bmatrix}_B$
Where $\begin{bmatrix}c_{1}\\\vdots\\c_{n}\end{bmatrix}$ is called the $B \text{-coordinate vector of x}$ represented as $\begin{bmatrix}x\end{bmatrix}_B$

- Let $T:V \to \mathbb{R}^{n}$ with a standard matrix $B$,
then,
- $x \to \begin{bmatrix}x\end{bmatrix}_B$ 
This is called coordinate mapping. I.e. we map all coordinates in $V$ space to those in $\mathbb{R}^{n}$ (or any other space for that matter) with a unique representation in terms of the coordinate vectors.

In other words, for each input, you get a single output. Hence, the transformation is a FUNCTION!

$T$ is a linear transformation.

IF $T$ is Bijective, i.e. one-to-one and onto (maps $V$ to the entire co-domain $\mathbb{R}^{n}$) then we say $T$ is a **Linear isomorphism** from $V$ onto $\mathbb{R}^{n}$

And there is a unique output for each $v \in V$ and there is a unique input $u \in V$ for all $v \in \mathbb{R}^{n}$.

Because of this bijective and linear mapping, the properties of $V$ are operationally identical to those of $\mathbb{R}^{n}$.

**You can still Change basis for a non-invertible $P_{b}$ though... at that point you will have a non-trivial null space.**
In other words, you can map points of a Higher dimensional Vector space to that of a lower dimensional Vector space. That is a valid transformation and change of coordinates however, that transformation would be non invertible since you would be mapping multiple points to the same value (as long as transformation is linear). Hence, it would be a many-to-one function. A non-bijective function is not invertible.

---
# Coordinates in $\mathbb{R}^n$
Let $B = \begin{bmatrix}b_1,\dots,b_n\end{bmatrix}$ be a basis for $\mathbb{R}^n$. Then for any $x \in \mathbb{R}^n$, there holds $$x = \begin{bmatrix}b_1,\dots,b_n\end{bmatrix}\,\begin{bmatrix}x\end{bmatrix}_B=P_B\,\begin{bmatrix}x\end{bmatrix}_B$$
$\begin{bmatrix}b_1,\dots,b_n\end{bmatrix}$ is the change-of-coordinates matrix denoted by $P_{b}$ and $\begin{bmatrix}x\end{bmatrix}_B$ is the coordinate vector (weights for each of the bases vectors in $P_{b}$) to each vector $x$.

Given that $B$ is a basis, $P_B$ is invertible (all of the columns are pivot columns) hence, $\begin{bmatrix}x\end{bmatrix}_{B} = P_B^{-1}x$ has a unique solution.
I.e. there is a unique pre-image for all images.

**The coordinate vector is the vector that we multiply with the change-of-coordinate matrix so as to get a particular point in the standard bases space**.

Recall ![[Maths/Intro to Linear Algebra/Lec 4. Matrix-Vector Multiplication#Change of Bases|Lec 4. Matrix-Vector Multiplication]]
![[Maths/Intro to Linear Algebra/attachments/linear algebra coordinate systems and basis.jpg|800]]

 You may not have an invertible transformation matrix. So you might not always be able to find the inverse matrix
 But really what you are doing is solving for $[x]_B$ in $P_{B}[x]_{B}=x$ hence, you can simply solve $[P_{b}|x]$.

##### Example 
$b_{1} = \begin{bmatrix}1\\2\\0\end{bmatrix}$, $b_{2} = \begin{bmatrix}0\\3\\1\end{bmatrix}$ and $x=\begin{bmatrix}1\\5\\1\end{bmatrix}$
solving $[P_{b}|x]$
we get $\begin{bmatrix}1&0&1\\0&1&1\\0&0&0\end{bmatrix}$
Since that is equivalent to saying $\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix} [x]_{B} = \begin{bmatrix}1\\1\\0\end{bmatrix}$,
$[x]_{B} = \begin{bmatrix}1\\1\end{bmatrix}$
---
